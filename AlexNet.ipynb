{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xingchenzhao/study_deep_learning/blob/master/AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIRC8AWvjVCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install d2lzh  # installing d2l\n",
        "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jnIhlQEjbgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import d2lzh as d2l\n",
        "from mxnet import gluon, init, nd\n",
        "from mxnet.gluon import data as gdata, nn\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXOuNx-OpUHm",
        "colab_type": "text"
      },
      "source": [
        "**Let's build a *AlexNet***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLInEki0jecb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential()\n",
        "net.add(nn.Conv2D(96, kernel_size=11, strides=4, activation='relu'),\n",
        "        nn.MaxPool2D(pool_size=3, strides=2),\n",
        "        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),\n",
        "        nn.MaxPool2D(pool_size=3, strides=2),\n",
        "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
        "        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),\n",
        "        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),\n",
        "        nn.MaxPool2D(pool_size=3, strides=2),\n",
        "        nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
        "        nn.Dense(4096, activation='relu'), nn.Dropout(0.5),\n",
        "        nn.Dense(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RPis-QZp6ip",
        "colab_type": "text"
      },
      "source": [
        "Build a 224*224 with 1 channel data to see the shape of each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC7ejdyqpkXk",
        "colab_type": "code",
        "outputId": "7a903a9e-8b09-41bb-d2e6-e27590485041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X = nd.random.uniform(shape=(1,1,224,224))\n",
        "net.initialize()\n",
        "for layer in net:\n",
        "  X = layer(X)\n",
        "  print(layer.name, 'output shape:\\t', X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv0 output shape:\t (1, 96, 54, 54)\n",
            "pool0 output shape:\t (1, 96, 26, 26)\n",
            "conv1 output shape:\t (1, 256, 26, 26)\n",
            "pool1 output shape:\t (1, 256, 12, 12)\n",
            "conv2 output shape:\t (1, 384, 12, 12)\n",
            "conv3 output shape:\t (1, 384, 12, 12)\n",
            "conv4 output shape:\t (1, 256, 12, 12)\n",
            "pool2 output shape:\t (1, 256, 5, 5)\n",
            "dense0 output shape:\t (1, 4096)\n",
            "dropout0 output shape:\t (1, 4096)\n",
            "dense1 output shape:\t (1, 4096)\n",
            "dropout1 output shape:\t (1, 4096)\n",
            "dense2 output shape:\t (1, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9I2cZQAqF24",
        "colab_type": "text"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-82QvhRqE1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n",
        "    '~','.mxnet', 'datasets', 'fashion-mnist')):\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "      transformer += [gdata.vision.transforms.Resize(resize)]\n",
        "    transformer += [gdata.vision.transforms.ToTensor()]\n",
        "    transformer = gdata.vision.transforms.Compose(transformer)\n",
        "    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n",
        "    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "    train_iter = gdata.DataLoader(\n",
        "          mnist_train.transform_first(transformer), batch_size, shuffle=True,\n",
        "          num_workers=num_workers)\n",
        "    test_iter = gdata.DataLoader(\n",
        "          mnist_test.transform_first(transformer), batch_size, shuffle=False,\n",
        "          num_workers=num_workers)\n",
        "    return train_iter, test_iter\n",
        "\n",
        "batch_size = 128\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPk_UChPr2LU",
        "colab_type": "text"
      },
      "source": [
        "Training AlexNet. We use a small learning rate due to the complexities of AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ5EJYJRr1lu",
        "colab_type": "code",
        "outputId": "e3282d7f-553d-42d7-d33c-dcf9be56d493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lr, num_epochs, ctx = 0.01, 100, d2l.try_gpu()\n",
        "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
        "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate':lr})\n",
        "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on gpu(0)\n",
            "epoch 1, loss 1.3014, train acc 0.510, test acc 0.752, time 36.4 sec\n",
            "epoch 2, loss 0.6470, train acc 0.759, test acc 0.811, time 33.9 sec\n",
            "epoch 3, loss 0.5284, train acc 0.805, test acc 0.843, time 33.9 sec\n",
            "epoch 4, loss 0.4650, train acc 0.830, test acc 0.857, time 34.0 sec\n",
            "epoch 5, loss 0.4254, train acc 0.845, test acc 0.867, time 33.9 sec\n",
            "epoch 6, loss 0.3941, train acc 0.858, test acc 0.872, time 33.8 sec\n",
            "epoch 7, loss 0.3722, train acc 0.863, test acc 0.885, time 34.1 sec\n",
            "epoch 8, loss 0.3530, train acc 0.872, test acc 0.879, time 33.9 sec\n",
            "epoch 9, loss 0.3413, train acc 0.876, test acc 0.891, time 34.1 sec\n",
            "epoch 10, loss 0.3278, train acc 0.880, test acc 0.891, time 34.0 sec\n",
            "epoch 11, loss 0.3163, train acc 0.884, test acc 0.895, time 34.0 sec\n",
            "epoch 12, loss 0.3076, train acc 0.887, test acc 0.897, time 34.1 sec\n",
            "epoch 13, loss 0.2981, train acc 0.891, test acc 0.898, time 33.9 sec\n",
            "epoch 14, loss 0.2890, train acc 0.893, test acc 0.901, time 34.2 sec\n",
            "epoch 15, loss 0.2822, train acc 0.897, test acc 0.899, time 34.0 sec\n",
            "epoch 16, loss 0.2747, train acc 0.899, test acc 0.906, time 34.1 sec\n",
            "epoch 17, loss 0.2671, train acc 0.902, test acc 0.908, time 33.8 sec\n",
            "epoch 18, loss 0.2601, train acc 0.904, test acc 0.910, time 33.7 sec\n",
            "epoch 19, loss 0.2546, train acc 0.907, test acc 0.906, time 33.8 sec\n",
            "epoch 20, loss 0.2484, train acc 0.908, test acc 0.910, time 33.9 sec\n",
            "epoch 21, loss 0.2437, train acc 0.910, test acc 0.911, time 34.0 sec\n",
            "epoch 22, loss 0.2377, train acc 0.913, test acc 0.913, time 33.9 sec\n",
            "epoch 23, loss 0.2330, train acc 0.914, test acc 0.910, time 33.9 sec\n",
            "epoch 24, loss 0.2266, train acc 0.917, test acc 0.914, time 34.0 sec\n",
            "epoch 25, loss 0.2219, train acc 0.917, test acc 0.917, time 33.9 sec\n",
            "epoch 26, loss 0.2176, train acc 0.918, test acc 0.917, time 33.8 sec\n",
            "epoch 27, loss 0.2116, train acc 0.922, test acc 0.917, time 33.8 sec\n",
            "epoch 28, loss 0.2072, train acc 0.923, test acc 0.917, time 33.9 sec\n",
            "epoch 29, loss 0.2020, train acc 0.925, test acc 0.919, time 33.8 sec\n",
            "epoch 30, loss 0.1981, train acc 0.926, test acc 0.920, time 33.5 sec\n",
            "epoch 31, loss 0.1943, train acc 0.928, test acc 0.920, time 33.5 sec\n",
            "epoch 32, loss 0.1896, train acc 0.930, test acc 0.918, time 33.7 sec\n",
            "epoch 33, loss 0.1833, train acc 0.931, test acc 0.919, time 33.6 sec\n",
            "epoch 34, loss 0.1779, train acc 0.934, test acc 0.923, time 33.7 sec\n",
            "epoch 35, loss 0.1759, train acc 0.935, test acc 0.922, time 33.7 sec\n",
            "epoch 36, loss 0.1706, train acc 0.937, test acc 0.924, time 33.8 sec\n",
            "epoch 37, loss 0.1660, train acc 0.937, test acc 0.917, time 33.7 sec\n",
            "epoch 38, loss 0.1642, train acc 0.938, test acc 0.923, time 33.8 sec\n",
            "epoch 39, loss 0.1582, train acc 0.941, test acc 0.924, time 33.7 sec\n",
            "epoch 40, loss 0.1556, train acc 0.942, test acc 0.925, time 33.8 sec\n",
            "epoch 41, loss 0.1508, train acc 0.943, test acc 0.923, time 33.7 sec\n",
            "epoch 42, loss 0.1472, train acc 0.945, test acc 0.925, time 33.8 sec\n",
            "epoch 43, loss 0.1445, train acc 0.946, test acc 0.925, time 33.8 sec\n",
            "epoch 44, loss 0.1402, train acc 0.947, test acc 0.926, time 34.0 sec\n",
            "epoch 45, loss 0.1328, train acc 0.949, test acc 0.928, time 33.8 sec\n",
            "epoch 46, loss 0.1285, train acc 0.952, test acc 0.926, time 33.8 sec\n",
            "epoch 47, loss 0.1288, train acc 0.952, test acc 0.921, time 34.0 sec\n",
            "epoch 48, loss 0.1254, train acc 0.953, test acc 0.927, time 33.9 sec\n",
            "epoch 49, loss 0.1189, train acc 0.955, test acc 0.927, time 33.8 sec\n",
            "epoch 50, loss 0.1149, train acc 0.956, test acc 0.927, time 33.9 sec\n",
            "epoch 51, loss 0.1136, train acc 0.957, test acc 0.929, time 33.8 sec\n",
            "epoch 52, loss 0.1104, train acc 0.958, test acc 0.922, time 34.1 sec\n",
            "epoch 53, loss 0.1057, train acc 0.960, test acc 0.927, time 33.9 sec\n",
            "epoch 54, loss 0.1024, train acc 0.961, test acc 0.926, time 33.9 sec\n",
            "epoch 55, loss 0.0983, train acc 0.963, test acc 0.927, time 33.8 sec\n",
            "epoch 56, loss 0.0947, train acc 0.964, test acc 0.929, time 33.8 sec\n",
            "epoch 57, loss 0.0907, train acc 0.966, test acc 0.926, time 33.9 sec\n",
            "epoch 58, loss 0.0908, train acc 0.966, test acc 0.931, time 33.9 sec\n",
            "epoch 59, loss 0.0871, train acc 0.967, test acc 0.927, time 33.8 sec\n",
            "epoch 60, loss 0.0843, train acc 0.968, test acc 0.926, time 33.9 sec\n",
            "epoch 61, loss 0.0820, train acc 0.969, test acc 0.926, time 34.0 sec\n",
            "epoch 62, loss 0.0772, train acc 0.971, test acc 0.924, time 33.9 sec\n",
            "epoch 63, loss 0.0778, train acc 0.970, test acc 0.927, time 33.7 sec\n",
            "epoch 64, loss 0.0732, train acc 0.973, test acc 0.928, time 33.7 sec\n",
            "epoch 65, loss 0.0709, train acc 0.974, test acc 0.928, time 33.7 sec\n",
            "epoch 66, loss 0.0679, train acc 0.974, test acc 0.929, time 33.7 sec\n",
            "epoch 67, loss 0.0675, train acc 0.975, test acc 0.925, time 33.7 sec\n",
            "epoch 68, loss 0.0638, train acc 0.976, test acc 0.932, time 33.8 sec\n",
            "epoch 69, loss 0.0594, train acc 0.978, test acc 0.929, time 33.7 sec\n",
            "epoch 70, loss 0.0608, train acc 0.977, test acc 0.929, time 33.6 sec\n",
            "epoch 71, loss 0.0577, train acc 0.978, test acc 0.932, time 33.8 sec\n",
            "epoch 72, loss 0.0556, train acc 0.979, test acc 0.927, time 33.7 sec\n",
            "epoch 73, loss 0.0525, train acc 0.981, test acc 0.931, time 33.8 sec\n",
            "epoch 74, loss 0.0523, train acc 0.981, test acc 0.930, time 33.7 sec\n",
            "epoch 75, loss 0.0505, train acc 0.982, test acc 0.928, time 33.6 sec\n",
            "epoch 76, loss 0.0471, train acc 0.983, test acc 0.931, time 33.8 sec\n",
            "epoch 77, loss 0.0485, train acc 0.982, test acc 0.928, time 33.7 sec\n",
            "epoch 78, loss 0.0444, train acc 0.983, test acc 0.930, time 33.7 sec\n",
            "epoch 79, loss 0.0445, train acc 0.983, test acc 0.931, time 33.7 sec\n",
            "epoch 80, loss 0.0410, train acc 0.985, test acc 0.929, time 33.7 sec\n",
            "epoch 81, loss 0.0396, train acc 0.986, test acc 0.931, time 33.8 sec\n",
            "epoch 82, loss 0.0388, train acc 0.985, test acc 0.929, time 33.7 sec\n",
            "epoch 83, loss 0.0381, train acc 0.987, test acc 0.925, time 33.7 sec\n",
            "epoch 84, loss 0.0382, train acc 0.986, test acc 0.929, time 33.8 sec\n",
            "epoch 85, loss 0.0336, train acc 0.987, test acc 0.926, time 33.7 sec\n",
            "epoch 86, loss 0.0333, train acc 0.988, test acc 0.930, time 33.7 sec\n",
            "epoch 87, loss 0.0309, train acc 0.989, test acc 0.928, time 33.7 sec\n",
            "epoch 88, loss 0.0318, train acc 0.989, test acc 0.931, time 33.7 sec\n",
            "epoch 89, loss 0.0308, train acc 0.989, test acc 0.929, time 33.7 sec\n",
            "epoch 90, loss 0.0299, train acc 0.989, test acc 0.928, time 33.8 sec\n",
            "epoch 91, loss 0.0290, train acc 0.990, test acc 0.931, time 33.6 sec\n",
            "epoch 92, loss 0.0304, train acc 0.989, test acc 0.927, time 33.7 sec\n",
            "epoch 93, loss 0.0279, train acc 0.990, test acc 0.930, time 33.7 sec\n",
            "epoch 94, loss 0.0257, train acc 0.990, test acc 0.932, time 33.6 sec\n",
            "epoch 95, loss 0.0249, train acc 0.991, test acc 0.929, time 33.8 sec\n",
            "epoch 96, loss 0.0241, train acc 0.992, test acc 0.928, time 33.6 sec\n",
            "epoch 97, loss 0.0242, train acc 0.991, test acc 0.930, time 33.7 sec\n",
            "epoch 98, loss 0.0230, train acc 0.992, test acc 0.931, time 33.6 sec\n",
            "epoch 99, loss 0.0225, train acc 0.992, test acc 0.929, time 33.7 sec\n",
            "epoch 100, loss 0.0224, train acc 0.992, test acc 0.931, time 33.6 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}