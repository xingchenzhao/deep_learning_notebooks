{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of multilayerPerception_Scratch .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xingchenzhao/study_deep_learning/blob/master/multilayerPerception_Scratch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0sxURZJRbdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install d2lzh  # installing d2l\n",
        "!pip install -U --pre mxnet-cu101mkl  # updating mxnet to at least v1.6\n",
        "\n",
        "%matplotlib inline\n",
        "import d2lzh as d2l\n",
        "from mxnet import nd\n",
        "from mxnet.gluon import loss as gloss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrtLfrb7TuHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load fashion mnist dataset\n",
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grt587mOUsBl",
        "colab_type": "text"
      },
      "source": [
        "Because the pixel of a picture from the dataset is 28*28 and the category number is 10, \n",
        "so we use 28*28 = 784 as the vector to show the picture. Thus, the input number 784, output number is 10.\n",
        "For experiment, we use 256 as the number of hidden layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwXpjd_tSriz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
        "\n",
        "W1 = nd.random.normal(scale = 0.01, shape = (num_inputs, num_hiddens))\n",
        "b1 = nd.zeros(num_hiddens)\n",
        "W2 = nd.random.normal(scale = 0.01, shape = (num_hiddens, num_outputs))\n",
        "b2 = nd.zeros(num_outputs)\n",
        "\n",
        "params = [W1,b1,W2,b2]\n",
        "for param in params:\n",
        "  param.attach_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVRbmBHvTmXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we use relu as the activation function\n",
        "def relu(X):\n",
        "  return nd.maximum(X,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8uXqVmCUw29",
        "colab_type": "text"
      },
      "source": [
        "Like the softmax regression, we can use reshape function to make the orginal picture to the length of the `num_inputs` vector. Then we realize the multilayer perception function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYblCukqUa32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net(X):\n",
        "  X = X.reshape((-1,num_inputs))\n",
        "  H = relu(nd.dot(X,W1)+b1)\n",
        "  return nd.dot(H,W2)+b2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNzzkF-_VKkq",
        "colab_type": "text"
      },
      "source": [
        "Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiNiYBtUVJfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = gloss.SoftmaxCrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdyMo3UEVYzE",
        "colab_type": "text"
      },
      "source": [
        "Train the model, we can simply call the training function from the d2l package as we have defined before.\n",
        "For here, we set the hyperparameter num_epoch to 5 and learning rate to 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOZpETuGVQK1",
        "colab_type": "code",
        "outputId": "131a68c3-4c05-4852-b24f-201fe0a7f1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "num_epochs, lr = 5, 0.5\n",
        "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 0.8017, train acc 0.698, test acc 0.777\n",
            "epoch 2, loss 0.4868, train acc 0.820, test acc 0.850\n",
            "epoch 3, loss 0.4297, train acc 0.840, test acc 0.861\n",
            "epoch 4, loss 0.3936, train acc 0.854, test acc 0.867\n",
            "epoch 5, loss 0.3750, train acc 0.862, test acc 0.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zVuMT82WWDB",
        "colab_type": "text"
      },
      "source": [
        "Summary:\n",
        "* We can manually define the model and realize a simple multilayer perception\n",
        "* When the num of layers is relatively high, the realization from this notebook will be complex, for example, when we define the hyperparameter."
      ]
    }
  ]
}